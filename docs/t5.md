# T5 in Triton

### 0. Build Env
```bash
# Create Triton IS
docker build -t llm_trt_exporter .
docker run --rm -it -v $PWD:/py -w /py --runtime nvidia llm_trt_exporter bash
```

### 1. Build Model (Hugging Face -> Pytorch -> ONNX -> TensorRT)

##### T5 ( NNDF )

```bash
export SRC_DIR=model_zoo/t5
python3 transform.py t5
```
</details>


#### 1.2 .ONNX -> TensorRT

t5 (encoder-decoder model)

```bash
trtexec --onnx=$SRC_DIR/encoder/model.onnx \
        --saveEngine=$SRC_DIR/encoder/model.plan \
        --minShapes=input_ids:1x1 \
        --optShapes=input_ids:8x256 \
        --maxShapes=input_ids:32x512 \
        --memPoolSize=workspace:2048 \
        --fp16

trtexec --onnx=$SRC_DIR/decoder/model.onnx \
        --saveEngine=$SRC_DIR/decoder/model.plan \
        --minShapes=input_ids:1x1,encoder_hidden_states:1x1x512 \
        --optShapes=input_ids:8x256,encoder_hidden_states:8x256x512 \
        --maxShapes=input_ids:32x512,encoder_hidden_states:32x512x512 \
        --memPoolSize=workspace:2048 \
        --fp16
```

### 2. Setup Triton Inference Server

```bash
export DST_DIR=triton-server/models_t5
# Copy Model to Triton
mv $SRC_DIR/encoder/model.plan             $DST_DIR/encoder/1/
mv $SRC_DIR/decoder/model.plan             $DST_DIR/decoder/1/
cp -r $SRC_DIR/config                      $DST_DIR/tokenizer/1/          # Copy Tokenizer to TRITON
cp -r $SRC_DIR/config                      $DST_DIR/tokenizer_out/1/      # Copy Tokenizer to TRITON

# copy config.pbtxt
cp config/t5/encoder_config.pbtxt               $DST_DIR/encoder/config.pbtxt
cp config/t5/decoder_config.pbtxt               $DST_DIR/decoder/config.pbtxt
cp config/t5/ensemble_config.pbtxt              $DST_DIR/llm/config.pbtxt
cp config/t5/tokenizer_config.pbtxt             $DST_DIR/tokenizer/config.pbtxt
cp config/t5/tokenizer_out_config.pbtxt         $DST_DIR/tokenizer_out/config.pbtxt

# Exit trt_builder
```

```bash
# Create Triton IS
#  *** Remember *** - change to mount triton_server/models_t5:/models
docker-compose build
docker-compose up
```

---

### 3. Client

#### Quick test
```bash
docker exec -it controller sh -c "python3 send_request.py -u triton:8000 --batch_size 8 -m llm -i TEXT -o OUTPUT" > log  && echo "" >> log
```
<details><summary> Manual test </summary>

####
```bash
docker exec -it controller bash
# testing / benchmark
perf_analyzer -m tokenizer      -u triton:8000 -i HTTP -v -p3000 -d -l3000 -t1 -c5 -b1 --string-data "Hello, I'm Machine Learning Engineer, my duty is " --shape text:1  # tokenizer
perf_analyzer -m encoder        -u triton:8000 -i HTTP -v -p3000 -d -l3000 -t1 -c5 -b1 --shape input_ids:128 # encoder model
perf_analyzer -m decoder        -u triton:8000 -i HTTP -v -p3000 -d -l3000 -t1 -c5 -b1 --shape input_ids:128 --shape encoder_hidden_states:128,512 # decoder model
perf_analyzer -m tokenizer_out  -u triton:8000 -i HTTP -v -p3000 -d -l3000 -t1 -c5 -b1 --shape decoder_hidden_states:128,32128 # tokenizer out
perf_analyzer -m llm            -u triton:8000 -i HTTP -v -p3000 -d -l3000 -t1 -c5 -b1 --string-data "Hello, I'm Machine Learning Engineer, my duty is " --shape TEXT:1  # ensemble
```

</details>

---

### 4. Clean up
```bash
# clean up Triton
sh cleanup.sh $DST_DIR
```